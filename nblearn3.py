import os
import sys
import glob

stop_word = ['on', 'this', 'with', 'they', 'our', 'have', 'from', 'there', 'for', 'it', 'at', 'that', 'a', 'an', 'the',
             'i', 'am', 'my', 'myself', 'she', 'her', 'he', 'his', 'is', 'am', 'are', 'was', 'we', 'you', 'were']

# Used to store corpus collected in training data
all_words = []

# Hyper_parameters
smoothing_rate = 0.2
most_delete = 15
least_delete = 30
parameter_degree = '0.8'


def openFolder():
    """
    Open the folder of Positive or Negative
    :return: path to txt file
    """
    # Initialize empty word bags for each label
    wb_positive = {}
    wb_negative = {}
    wb_truth = {}
    wb_deceptive = {}

    root_path = "/Users/jinkunluo/Downloads/op_spam_training_data"

    all_path = ["/negative_polarity/deceptive_from_MTurk", "/negative_polarity/truthful_from_Web",
                "/positive_polarity/deceptive_from_MTurk", "/positive_polarity/truthful_from_TripAdvisor"]
    for parent_path in all_path:
        for i in range(1, 5):
            total_path = root_path + parent_path + '/fold' + str(i)
            f = glob.glob(total_path + '/*.txt')
            for curr_path in f:
                with open(curr_path, 'r') as txt:
                    curr_text = txt.read()
                    if "positive" in curr_path:
                        token_String(curr_text, wb_positive)
                    if "negative" in curr_path:
                        token_String(curr_text, wb_negative)
                    if "truthful" in curr_path:
                        token_String(curr_text, wb_truth)
                    if "deceptive" in curr_path:
                        token_String(curr_text, wb_deceptive)
                print("aaa", wb_positive, wb_negative, wb_truth, wb_deceptive)
    return wb_positive, wb_negative, wb_truth, wb_deceptive


def token_String(text, word_bag):
    """
    Token each document.
    Give string, split it into dictionary. --> {word:count}
    Convert uppercase to lowercase.
    :param word_bag: store the word bag already loaded from text file before
    :param text: String
    :return: word-bag generated by text --> Dictionary
    """
    rest_text = text.lower()

    # Store all words in given text --> {word:count}
    # word_bag = {}
    # Record current word
    tmp_word = ''
    for c in rest_text:
        if c.isalpha():
            tmp_word += c
        elif tmp_word.isalpha():
            if tmp_word in word_bag.keys():
                word_bag[tmp_word] += 1
            else:
                word_bag[tmp_word] = 1
            tmp_word = ''
        else:
            tmp_word = ''
    # print(word_bag)
    return word_bag


def delete_stop_word(word_bag):
    """
    Process word_bag of each class.
    Delete top 10 most common words and add them into stop words.
    Delete stop words from word_bag.
    :param word_bag:
    :return: new word bag of each label
    """
    sort_l = sorted(word_bag.items(), key=lambda x: x[1], reverse=True)

    # Add most popular 10 words to stop word list
    most_word = sort_l[:most_delete]
    for w in most_word:
        stop_word.append(list(w)[0])
    least_word = sort_l[-least_delete:]
    for w in least_word:
        stop_word.append(list(w)[0])
    set(stop_word)
    print(stop_word)

    # Delete words in word bag according to stop words set
    print("word_bag", word_bag)
    new_word_bag = {}
    for k in word_bag.keys():
        if k not in stop_word:
            new_word_bag[k] = word_bag[k]
            # Record words into all words
            all_words.append(k)
    list(set(all_words))
    print(all_words)

    return new_word_bag


def get_Model():
    wb_p, wb_n, wb_t, wb_d = openFolder()
    print("bbb", wb_p, wb_n, wb_d, wb_t)

    prob_p = calculate_Probability(delete_stop_word(wb_p))
    prob_n = calculate_Probability(delete_stop_word(wb_n))
    prob_t = calculate_Probability(delete_stop_word(wb_t))
    prob_d = calculate_Probability(delete_stop_word(wb_d))

    model = {}
    for w in prob_p.keys():
        model[w] = [prob_p[w], prob_n[w], prob_t[w], prob_d[w]]
    return model


def calculate_Probability(word_bag):
    """
    Calculate P(xi|class) = count(xi)/total words in the class after delete stop words
    Smoothing: 将所有该类中没有的词算作count=0.5，所以total words += 0.5*这类没有的词个数
    Smoothing rate : 0.5 ???
    :param word_bag: of each class
    :return: Dictionary --> conditional probability of each feature
    """
    conditional_prob = {}
    count_smoothing = len(all_words) - len(word_bag.keys())
    total_words = sum(word_bag.values()) + count_smoothing * smoothing_rate
    for k in all_words:
        if k in word_bag.keys():
            conditional_prob[k] = word_bag[k] / total_words
        else:
            conditional_prob[k] = smoothing_rate / total_words
    return conditional_prob


def calculate_accuracy(root_path, train_model):
    """
    Transverse all of .txt files to calculate labels
    :return: train accuracy
    """
    total_correct1 = 0  # positive or negative
    total_correct2 = 0  # truthful or deceptive
    total_file = 1280  # 2*2*4*80

    # train_model = get_Model(root_path)
    for filename in os.walk(root_path):
        # transverse all file and file folder
        # filename : String-path list<String>-file_folder_name list<String>-file_name
        filename = list(filename)
        if len(filename[1]) == 0:
            for txt_name in filename[2]:
                # transverse all of .txt file
                curr_path = filename[0] + '/' + txt_name
                with open(curr_path, 'r') as txt:
                    curr_text = txt.read()
                    curr_words = token_String(curr_text, {})
                    p_positive = 0
                    p_negative = 0
                    p_truthful = 0
                    p_deceptive = 0
                    for w, v in curr_words.items():
                        if w in train_model.keys():
                            p_positive += v * train_model[w][0]
                            p_negative += v * train_model[w][1]
                            p_truthful += v * train_model[w][2]
                            p_deceptive += v * train_model[w][3]
                    if p_positive > p_negative and "positive" in curr_path:
                        total_correct1 += 1
                    if p_positive < p_negative and "negative" in curr_path:
                        total_correct1 += 1
                    if p_truthful > p_deceptive and "truthful" in curr_path:
                        total_correct2 += 1
                    if p_truthful < p_deceptive and "deceptive" in curr_path:
                        total_correct2 += 1
    accuracy1 = total_correct1 / total_file
    accuracy2 = total_correct2 / total_file

    return accuracy1, accuracy2


def writeOutput(root_path, output_name, train_model):
    complete_path = root_path + '/' + output_name

    with open(complete_path, 'w') as f:
        f.write("")
        
    with open(complete_path, 'a') as f:
        for k in train_model.keys():
            tmp_list = train_model[k]
            tmp_txt = k + '\n\tPositive:' + str(format(tmp_list[0], parameter_degree+'f')) + '\tNegative:' + str(format(tmp_list[1], parameter_degree+'f')) + '\tTruthful:' + str(format(tmp_list[2], parameter_degree+'f')) + '\tDeceptive:' + str(format(tmp_list[3], parameter_degree+'f')) +'\n'
            f.write(tmp_txt)


def main(argv):
    # For each file, read it
    # files = [''];
    # for file in files:
    #     readInput(file)
    # Open all
    # root_path = "/Users/jinkunluo/Downloads/op_spam_training_data"
    root_path = "/Users/jinkunluo/Downloads/op_spam_training_data"
    output_path = "nbmodel.txt"
    model = get_Model()
    # accuracy1, accuracy2 = calculate_accuracy(root_path, model)
    # print(calculate_accuracy(root_path))
    writeOutput(root_path, output_path, model)
    # wb_p, wb_n, wb_t, wb_d = openFolder(root_path)

    # ave_count_p = sum(wb_p.values()) / 640
    # ave_count_n = sum(wb_n.values()) / 640
    # ave_count_t = sum(wb_t.values()) / 640
    # ave_count_d = sum(wb_d.values()) / 640
    # print(str(ave_count_p), str(ave_count_n), str(ave_count_t), str(ave_count_d))
    #
    # prob_p = calculate_Probability(delete_stop_word(wb_p))
    # prob_n = calculate_Probability(delete_stop_word(wb_n))
    # prob_t = calculate_Probability(delete_stop_word(wb_t))
    # prob_d = calculate_Probability(delete_stop_word(wb_d))
    # print(prob_p)
    # print(prob_n)
    # print(prob_t)
    # print(prob_d)
    # l1 = sorted(prob_p.items(), key=lambda x: x[1], reverse=True)
    # l2 = sorted(prob_n.items(), key=lambda x: x[1], reverse=True)
    # l3 = sorted(prob_t.items(), key=lambda x: x[1], reverse=True)
    # l4 = sorted(prob_d.items(), key=lambda x: x[1], reverse=True)
    # print(l1)
    # print(l2)
    # print(l3)
    # print(l4)

    # print(sorted(prob_p.items(), key=lambda x: x[1], reverse=True))
    # print(sorted(prob_n.items(), key=lambda x: x[1], reverse=True))
    # print(sorted(prob_t.items(), key=lambda x: x[1], reverse=True))
    # print(sorted(prob_d.items(), key=lambda x: x[1], reverse=True))
    # print(str(ave_count_p), str(ave_count_n), str(ave_count_t), str(ave_count_d))


if __name__ == "__main__":
    main(sys.argv)
